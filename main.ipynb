{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "import inspect_ai\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from bert_score import score\n",
    "from typing import Any\n",
    "from matplotlib import pyplot as plt\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai import Task, task\n",
    "from inspect_ai.dataset import Sample, csv_dataset\n",
    "from inspect_ai.model import GenerateConfig\n",
    "from inspect_ai.scorer import choice\n",
    "from inspect_ai.solver import multiple_choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_questions(questions, dev_prompt, user_prompt):\n",
    "    '''\n",
    "    questions: list / np array of the questions in gpqa diamond\n",
    "    '''\n",
    "    summarized = []\n",
    "\n",
    "    for question in tqdm(questions):\n",
    "        input_text = (f\"{user_prompt}\\n<question>{question}</question>\")\n",
    "        \n",
    "        completion = openai.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"developer\", \"content\": f\"{dev_prompt}\"},\n",
    "                {\"role\": \"user\", \"content\": input_text}\n",
    "            ],\n",
    "            temperature=0.5,\n",
    "        )\n",
    "\n",
    "        model_output = completion.choices[0].message.content\n",
    "        summarized.append(model_output)\n",
    "    \n",
    "    return summarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('gpqa/gpqa_diamond.csv')\n",
    "questions = dataset['Question'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_prompt = 'Your task is to summarize the input question aggressively, while not losing any crucial information.'\n",
    "user_prompt = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized = summarize_questions(questions, dev_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Summarized Question'] = summarized\n",
    "dataset.to_csv('gpqa/updated_gpqa_diamond.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('gpqa/gpqa_diamond.csv')\n",
    "questions = dataset['Question'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def gpqa_diamond(cot: bool = True) -> Task:\n",
    "    return Task(\n",
    "        dataset=csv_dataset(\n",
    "            csv_file=\"gpqa/updated_gpqa_diamond.csv\",\n",
    "            sample_fields=record_to_sample,\n",
    "        ),\n",
    "        solver=[\n",
    "            multiple_choice(shuffle=True, cot=cot),\n",
    "        ],\n",
    "        scorer=choice(),\n",
    "        config=GenerateConfig(temperature=0.5),\n",
    "        epochs=1,\n",
    "    )\n",
    "\n",
    "def record_to_sample(record: dict[str, Any]) -> Sample:\n",
    "    return Sample(\n",
    "        input=record[\"Question\"],\n",
    "        choices=[\n",
    "            str(record[\"Correct Answer\"]),\n",
    "            str(record[\"Incorrect Answer 1\"]),\n",
    "            str(record[\"Incorrect Answer 2\"]),\n",
    "            str(record[\"Incorrect Answer 3\"]),\n",
    "        ],\n",
    "        target=\"A\",\n",
    "        id=record[\"Record ID\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def gpqa_diamond_summarized(cot: bool = True) -> Task:\n",
    "    return Task(\n",
    "        dataset=csv_dataset(\n",
    "            csv_file=\"gpqa/updated_gpqa_diamond.csv\",\n",
    "            sample_fields=record_to_sample_summarized,\n",
    "        ),\n",
    "        solver=[\n",
    "            multiple_choice(shuffle=True, cot=cot),\n",
    "        ],\n",
    "        scorer=choice(),\n",
    "        config=GenerateConfig(temperature=0.5),\n",
    "        epochs=1,\n",
    "    )\n",
    "\n",
    "def record_to_sample_summarized(record: dict[str, Any]) -> Sample:\n",
    "    return Sample(\n",
    "        input=record[\"Summarized Question\"],\n",
    "        choices=[\n",
    "            str(record[\"Correct Answer\"]),\n",
    "            str(record[\"Incorrect Answer 1\"]),\n",
    "            str(record[\"Incorrect Answer 2\"]),\n",
    "            str(record[\"Incorrect Answer 3\"]),\n",
    "        ],\n",
    "        target=\"A\",\n",
    "        id=record[\"Record ID\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_ai.eval(gpqa_diamond(), model='openai/o3-mini', epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results / Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_qs = tuple(zip(questions, summarized, dataset['Record ID'].to_list()))\n",
    "questions_ord, summarized_ord, ids_ord = np.array(sorted(zipped_qs, key=lambda qs : qs[2])).T.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntactic_summarization(original, summarized):\n",
    "    syntactic_sum = [len(s.split()) / len(q.split()) for q, s in zip(original, summarized)]\n",
    "    return syntactic_sum\n",
    "\n",
    "def semantic_summarization(original, summarized):\n",
    "    semantic_sum = score(summarized, original, lang='en')[2].tolist()\n",
    "    return semantic_sum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntactic_sum = np.array(syntactic_summarization(questions_ord, summarized_ord))\n",
    "semantic_sum = np.array(semantic_summarization(questions_ord, summarized_ord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4o_baseline_log = inspect_ai.log.read_eval_log('logs/gpt_4o_mini_baseline.eval')\n",
    "gpt4o_summarized_log = inspect_ai.log.read_eval_log('logs/gpt_4o_mini_summarized.eval')\n",
    "\n",
    "o3_baseline_log = inspect_ai.log.read_eval_log('logs/o3_mini_baseline.eval')\n",
    "o3_summarized_log = inspect_ai.log.read_eval_log('logs/o3_mini_summarized.eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_to_score(sample):\n",
    "    ans = sample.scores['choice'].value\n",
    "    return 1 if ans=='C' else 0\n",
    "\n",
    "gpt4o_baseline_scores = np.fromiter(map(sample_to_score, gpt4o_baseline_log.samples), int)\n",
    "gpt4o_summarized_scores = np.fromiter(map(sample_to_score, gpt4o_summarized_log.samples), int)\n",
    "\n",
    "o3_baseline_scores = np.fromiter(map(sample_to_score, o3_baseline_log.samples), int)\n",
    "o3_summarized_scores = np.fromiter(map(sample_to_score, o3_summarized_log.samples), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(bins, summarization, b_scores, s_scores):\n",
    "    '''\n",
    "    bins : summarization bins - how much of the original question was summarized\n",
    "    summarization : metric of summarization ('semantic sum' or 'syntactic sum')\n",
    "    b_scores : baseline scores \n",
    "    s_scores : summarized scores\n",
    "\n",
    "    returns model performance by bins\n",
    "    '''\n",
    "    assert len(b_scores) % len(summarization) == 0\n",
    "    \n",
    "    epochs = int(len(b_scores) / len(summarization))\n",
    "    summarization = einops.repeat(summarization, 'd -> (d e)', e=epochs)\n",
    "    num_bins = len(bins) - 1 \n",
    "\n",
    "    indices = np.digitize(summarization, bins)\n",
    "    test_arr = np.stack(arrays=[indices, b_scores, s_scores], axis=1)\n",
    "    binned_results = {\n",
    "        'Baseline Accuracy' : np.zeros(num_bins), \n",
    "        'Summarized Accuracy' : np.zeros(num_bins), \n",
    "        'N' : np.zeros(num_bins),\n",
    "    }\n",
    "\n",
    "    for i in range(num_bins):\n",
    "        binned_list = np.array([[b, s] for (j, b, s) in test_arr if j==i+1])\n",
    "        accuracy = np.sum(binned_list, axis=0) / binned_list.shape[0]\n",
    "\n",
    "        binned_results['Baseline Accuracy'][i] = accuracy[0]\n",
    "        binned_results['Summarized Accuracy'][i] = accuracy[1]\n",
    "        binned_results['N'][i] = binned_list.shape[0] / epochs\n",
    "\n",
    "    return binned_results\n",
    "\n",
    "syntactic_bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2]\n",
    "semantic_bins = [0.8, 0.85, 0.9, 0.95, 1.0]\n",
    "\n",
    "syntactic_stats = discretize(syntactic_bins, syntactic_sum, gpt4o_baseline_scores, gpt4o_summarized_scores)\n",
    "semantic_stats = discretize(semantic_bins, semantic_sum, gpt4o_baseline_scores, gpt4o_summarized_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_to_mean(scores):\n",
    "    mean = np.round(np.mean(scores * 100), 2)\n",
    "    return f'{mean}%'\n",
    "\n",
    "def log_to_ci(log):\n",
    "    stderr = log.results.scores[0].metrics['stderr'].value * 100\n",
    "    ci = np.round(1.96*stderr, 2)\n",
    "    return f'\\n± {ci}%'\n",
    "\n",
    "data = [\n",
    "    [\n",
    "        scores_to_mean(gpt4o_baseline_scores) + log_to_ci(gpt4o_baseline_log),\n",
    "        scores_to_mean(gpt4o_summarized_scores) + log_to_ci(gpt4o_summarized_log),\n",
    "    ],\n",
    "    [\n",
    "        scores_to_mean(o3_baseline_scores) + log_to_ci(o3_baseline_log),\n",
    "        scores_to_mean(o3_summarized_scores) + log_to_ci(o3_summarized_log),\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_labels = ['Baseline', 'Summarized \\nQuestions']\n",
    "row_labels = ['GPT 4o-mini', 'o3-mini']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4.5, 3))\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=data, rowLabels=row_labels, colLabels=col_labels,\n",
    "                 loc='center', cellLoc='center')\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "table.scale(1, 3.5)\n",
    "\n",
    "plt.title(\"LLM Performance on GPQA Diamond\", fontsize=14)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntactic_labels = ['0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0', '1.0-1.2']\n",
    "semantic_labels = ['0.80-0.85', '0.85-0.90', '0.90-0.95', '0.95-1.00']         \n",
    "# note: I removed bins that didn't have a representative number of samples\n",
    "\n",
    "baseline_acc = semantic_stats['Baseline Accuracy']\n",
    "summarized_acc = semantic_stats['Summarized Accuracy']\n",
    "\n",
    "x = np.arange(len(semantic_labels))\n",
    "bar_width = 0.40 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "rects1 = ax.bar(x - bar_width/2, baseline_acc, bar_width, label='Score on Original Qs', color='steelblue')\n",
    "rects2 = ax.bar(x + bar_width/2, summarized_acc, bar_width, label='Score on Summarized Qs', color='skyblue')\n",
    "\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel('Semantic Summarization')\n",
    "ax.set_ylabel('GPQA Score')\n",
    "ax.set_title('GPT 4o-mini on Summarized GPQA Questions')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(semantic_labels)\n",
    "ax.legend()\n",
    "\n",
    "for i in range(len(semantic_labels)):\n",
    "    ax.text(x[i], 0.1, f'n={semantic_stats[\"N\"][i]}', ha='center', fontsize=10, color='black', style='italic')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci1470",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
